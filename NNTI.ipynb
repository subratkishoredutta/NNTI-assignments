{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94943196",
   "metadata": {},
   "source": [
    "names: Subrat Kishore Dutta , Prathvish Mithare\n",
    "studentID: 7028082, 7028692\n",
    "email: subratkishoredutta1234@gmail.com, prathvishmithare7@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae3a63d",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "dd8f813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def matrixfunc():\n",
    "    random = np.random.rand(4,4)\n",
    "    trans = random.transpose()\n",
    "    inverse = np.linalg.inv(trans)\n",
    "    print(inverse)\n",
    "    return inverse\n",
    "def eigmat():\n",
    "    random = np.random.rand(4,4)\n",
    "    w, v = np.linalg.eig(random)\n",
    "    print(\"eigen value\",w)\n",
    "    print(\"eigen vector\",v)\n",
    "    return w,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "id": "becedec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.93661054  1.66428382  3.929769   -1.73862477]\n",
      " [ 0.62452847 -0.43068264 -1.95008776  2.09349729]\n",
      " [ 1.79269745 -1.077603   -0.51994879  0.18396553]\n",
      " [ 1.09060329  0.54137516 -1.46025687 -0.19368226]]\n",
      "eigen value [ 2.34634463+0.j         -0.41335836+0.29643305j -0.41335836-0.29643305j\n",
      "  0.21733603+0.j        ]\n",
      "eigen vector [[ 0.5140693 +0.j          0.24158654-0.50695888j  0.24158654+0.50695888j\n",
      "  -0.07233623+0.j        ]\n",
      " [ 0.64668786+0.j         -0.41627784+0.23902478j -0.41627784-0.23902478j\n",
      "  -0.7967314 +0.j        ]\n",
      " [ 0.3898993 +0.j          0.65218427+0.j          0.65218427-0.j\n",
      "   0.20286588+0.j        ]\n",
      " [ 0.4068244 +0.j         -0.13260429+0.10620892j -0.13260429-0.10620892j\n",
      "   0.56465208+0.j        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 2.34634463+0.j        , -0.41335836+0.29643305j,\n",
       "        -0.41335836-0.29643305j,  0.21733603+0.j        ]),\n",
       " array([[ 0.5140693 +0.j        ,  0.24158654-0.50695888j,\n",
       "          0.24158654+0.50695888j, -0.07233623+0.j        ],\n",
       "        [ 0.64668786+0.j        , -0.41627784+0.23902478j,\n",
       "         -0.41627784-0.23902478j, -0.7967314 +0.j        ],\n",
       "        [ 0.3898993 +0.j        ,  0.65218427+0.j        ,\n",
       "          0.65218427-0.j        ,  0.20286588+0.j        ],\n",
       "        [ 0.4068244 +0.j        , -0.13260429+0.10620892j,\n",
       "         -0.13260429-0.10620892j,  0.56465208+0.j        ]]))"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrixfunc()\n",
    "eigmat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58fe77",
   "metadata": {},
   "source": [
    "## Exercise 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "835cf978",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the required libraries\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms.functional import resize \n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "c08f0f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the customdataset\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self,image_paths,labels):\n",
    "        self.labels = labels\n",
    "        self.image_paths=image_paths \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self,id,transform=None):\n",
    "        label = self.labels[id]\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[id]),cv2.COLOR_BGR2RGB)\n",
    "        image=np.transpose(image,(2,0,1)).astype(np.float32)#(2,0,1) changes the format to (channel,width,height) from (width,height,channels)\n",
    "        return torch.tensor(image),label#torch.tensor(image),\n",
    "\n",
    "\n",
    "class customDataLoader(DataLoader):\n",
    "    def __init__(self,customdata,batch_size,collate_fn=None):\n",
    "        self.customdata=customdata\n",
    "        self.batch_size=batch_size\n",
    "        self.collate_fn=collate_fn\n",
    "\n",
    "    def loader(self):\n",
    "        if collate_fn!=None:\n",
    "            return DataLoader(self.customdata,self.batch_size,collate_fn=self.collate_fn)\n",
    "        elif collate_fn == None:\n",
    "            return DataLoader(self.customdata,self.batch_size)\n",
    "\n",
    "def custom_collate(batch):\n",
    "    instance=batch[0][0].size()[2]\n",
    "    h_resized=int(instance*0.8)\n",
    "    w_resized=int(instance*0.8)\n",
    "    image = [torchvision.transforms.Resize((int(h_resized),int(w_resized)))(sample[0]) for sample in batch] #resizing the images\n",
    "    b = torch.Tensor(1,3, h_resized, w_resized)\n",
    "    image=torch.stack(image, out=b) #convering the list of tensors into a single tensors\n",
    "    labels = [sample[1] for sample in batch] \n",
    "    return image,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "bdc8fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we are using a custom dataset from our local system with its local path\n",
    "base='D:/Saarland University/NNTI/data/' # path to the image directory\n",
    "images=os.listdir('D:/Saarland University/NNTI/data') # list of the image file names\n",
    "for i in range(len(images)):\n",
    "    images[i]=base+images[i] ## paths of the images in the local system \n",
    "labels=[]\n",
    "for i in range(1000):\n",
    "    labels.append(0) #defining dummy lebels \n",
    "\n",
    "customdata=customDataset(images,labels)# loading the custom data\n",
    "\n",
    "#creating objects of the class customDataLoader with the custom_collate function.\n",
    "customloader1=customDataLoader(customdata,batch_size=10,collate_fn=custom_collate)\n",
    "#creating objects of the class customDataLoader without the custom_collate function.\n",
    "customloader2=customDataLoader(customdata,batch_size=10,collate_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "2828928d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "#unpacking the data with a batch size of 10 with the original size of the image\n",
    "x,y=next(iter(customloader2.loader()))\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "42926bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: An output with one or more elements was resized since it had shape [1, 3, 409, 409], which does not match the required output shape [10, 3, 409, 409].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 409, 409])\n"
     ]
    }
   ],
   "source": [
    "#unpacking the data with a batch size of 10 with the 80% of the original size of the image\n",
    "x,y=next(iter(customloader1.loader()))\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "7bf52a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 409, 409])"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba19dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
