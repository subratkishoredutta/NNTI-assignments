{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef24d45",
   "metadata": {},
   "source": [
    "Name: Subrat Kishore Dutta  \n",
    "Matrikelnummer:  7028082\n",
    "Email:   subratkishoredutta1234@gmail.com,sudu00001@stud.uni-saarland.de\n",
    "   \n",
    "Name:   Prathvish Mithare\n",
    "Matrikelnummer:   7028692\n",
    "Email: prmi00001@stud.uni-saarland.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8990025",
   "metadata": {},
   "source": [
    "#### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482e0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import F1Score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e6b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c5fd97",
   "metadata": {},
   "source": [
    "# 7.5 Build your own regularized NN\n",
    "\n",
    "In this exercise you get to use your previously built networks, but this time you need to add regularization in the form of dropout and $L_2$-regularization.\n",
    "\n",
    "Each layer has the option of using dropout. Your code needs to allow for this flexibility.\n",
    "\n",
    "Additionally, adding $L_2$-regularization should also be optional upon creation.\n",
    "\n",
    "**NOTE**: You are allowed to use built-in functions from pytorch to incorporate this functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989729b",
   "metadata": {},
   "source": [
    "### 7.5.1 Implement a regularized model (1 point)\n",
    "\n",
    "Implement your own model (using `torch`) using the skeleton code provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dee1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement a model that incorporates dropout and L2 regularization\n",
    "    depending on arguments passed.\n",
    "    \n",
    "    Args:\n",
    "    input_dim: dimensionality of the inputs\n",
    "    hidden_dim: how many units each hidden layer will have\n",
    "    out_dim: how many output units\n",
    "    num_layers: how many hidden layers to create/use\n",
    "    dropout: a list of booleans specifying which hidden layers will have dropout\n",
    "    dropout_p: the probability used for the `Dropout` layers\n",
    "    l2_reg: a boolean value that indicates whether L2 regularization should be used\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, out_dim: int, num_layers: int, dropout: list, dropout_p: float,\n",
    "                 l2_reg: bool):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.dropout_p = dropout_p\n",
    "        self.l2_reg = l2_reg\n",
    "        self.fci = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.fch = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.fco = nn.Linear(self.hidden_dim,self.out_dim)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.m = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "        \n",
    "    def __call__(self,x):\n",
    "        out = self.fci(x)\n",
    "        out = F.relu(out)\n",
    "        #print(self.out)\n",
    "        for num in range(self.num_layers):\n",
    "            out = self.fch(out)\n",
    "            out = F.relu(out)\n",
    "            if self.dropout[num]:\n",
    "                out = self.m(out)\n",
    "        out = self.fco(out)\n",
    "        #out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "    def train(self,train_loader,learning_rate=0.01,epochs=5,lam=0.01):\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr=learning_rate)\n",
    "        ep=[]\n",
    "        lossrec=[]\n",
    "        for epoch in range(epochs):\n",
    "            tloss=0\n",
    "            ep.append(epoch)\n",
    "            for i,(xs,ys) in enumerate(train_loader):\n",
    "                xs=xs.to(device)\n",
    "                ys=ys.to(device)\n",
    "                pred = self(xs.view(-1,28*28))\n",
    "                if self.l2_reg:\n",
    "                    a=torch.tensor(0.).to(device)\n",
    "                    for p in self.parameters():\n",
    "                        a+=torch.norm(p)\n",
    "                    loss = loss_fn(pred,ys)+lam*a\n",
    "                else:\n",
    "                    loss = loss_fn(pred,ys)\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tloss+=loss\n",
    "            lossrec.append((tloss/len(train_loader)).log().item())\n",
    "            print('epoch:',epoch,'loss:',(tloss/len(train_loader)).item())\n",
    "        \n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.plot(ep,lossrec,color='orange')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('log loss')\n",
    "        return lossrec\n",
    "            \n",
    "    def test(self,test_loader):\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        loss=0\n",
    "        for i,(xs,ys) in enumerate(test_loader):\n",
    "            xs=xs.to(device)\n",
    "            ys=ys.to(device)\n",
    "            pred = self(xs.view(-1,28*28))\n",
    "            loss+= loss_fn(pred,ys).item()\n",
    "        print(loss/len(test_loader))\n",
    "        return loss/len(test_loader)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce63778",
   "metadata": {},
   "outputs": [],
   "source": [
    "##function to get accuracy:\n",
    "def get_accuracy(data,model):\n",
    "    accdata=torch.utils.data.DataLoader(data,batch_size=len(data))\n",
    "    for X,Y in accdata:\n",
    "        X=X.to(device)\n",
    "        Y=Y.to(device)\n",
    "        ypred=model(X.view(-1,28*28))\n",
    "        train_acc = torch.sum(ypred.argmax(1) == Y)\n",
    "        accuracy = train_acc/len(data)\n",
    "    print(accuracy.item()*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c30eb2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(data,model):\n",
    "    accdata=torch.utils.data.DataLoader(data,batch_size=len(data))\n",
    "    for X,Y in accdata:\n",
    "        X=X.to(device)\n",
    "        Y=Y.to(device)\n",
    "        f1= F1Score(task=\"multiclass\", num_classes=10)\n",
    "        ypred=model(X.view(-1,28*28))\n",
    "        f1 = f1(ypred.argmax(1),Y)\n",
    "    print(\"F1 score:\",f1.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c3b3c",
   "metadata": {},
   "source": [
    "### 7.5.2 Experiment with your model (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2248a4e",
   "metadata": {},
   "source": [
    "Use the MNIST dataset and evaluation code from the previous assignment to run some experiments. Run the following experiments:\n",
    "\n",
    "1. Shallow network (not more than 1 hidden layer)\n",
    "1. Shallow regularized network\n",
    "1. Deep network (at least 3 hidden layers)\n",
    "1. Deep regularized network\n",
    "\n",
    "Report Accuracy and $F_1$ metrics for your experiments and discuss your results. What did you expect to see and what did you end up seeing.\n",
    "\n",
    "**NOTE**: You can choose how you use regularization. Ideally you would experiment with various parameters for this regularization, the 4 listed variants are merely what you must cover as a minimum. Report results for all your experiments concisely in a table.\n",
    "\n",
    "**NOTE 2**: Make sure to report your metrics on the training and evaluation/heldout sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fa5c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# DO NOT CHANGE THE CODE IN THIS CELL EXCEPT FOR THE BATCH SIZE IF NECESSARY\n",
    "transform_fn = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.7,), (0.7,)),])\n",
    "\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform_fn)\n",
    "train_dl = torch.utils.data.DataLoader(mnist_train, batch_size=128, shuffle=True)\n",
    "\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform_fn)\n",
    "test_dl = torch.utils.data.DataLoader(mnist_test, batch_size=128, shuffle=False)\n",
    "\n",
    "# Use the above data for your experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce4df9e",
   "metadata": {},
   "source": [
    "### Experiments:\n",
    "\n",
    "#### 1.Choosing the correct learning rate\n",
    "\n",
    "We took a shallow model to test a range of learning rates within a range and choose the best performing one going ahead to train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= tensor(1.0000e-04)\n",
      "epoch: 0 loss: 0.7344396710395813\n",
      "epoch: 1 loss: 0.3338369131088257\n",
      "epoch: 2 loss: 0.2800363302230835\n",
      "epoch: 3 loss: 0.2392803430557251\n",
      "epoch: 4 loss: 0.20572535693645477\n",
      "epoch: 5 loss: 0.1783585101366043\n",
      "epoch: 6 loss: 0.15549670159816742\n",
      "epoch: 7 loss: 0.1395590901374817\n",
      "epoch: 8 loss: 0.12360269576311111\n",
      "epoch: 9 loss: 0.11185803264379501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▌                                                                              | 1/50 [02:26<1:59:28, 146.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= tensor(0.0001)\n",
      "epoch: 0 loss: 0.7074087262153625\n",
      "epoch: 1 loss: 0.325470894575119\n",
      "epoch: 2 loss: 0.26737648248672485\n",
      "epoch: 3 loss: 0.22245457768440247\n",
      "epoch: 4 loss: 0.19126781821250916\n",
      "epoch: 5 loss: 0.163621187210083\n",
      "epoch: 6 loss: 0.14394983649253845\n",
      "epoch: 7 loss: 0.12815718352794647\n",
      "epoch: 8 loss: 0.11349968612194061\n",
      "epoch: 9 loss: 0.10290881246328354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|███▏                                                                            | 2/50 [04:55<1:57:38, 147.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= tensor(0.0001)\n",
      "epoch: 0 loss: 0.6690120100975037\n",
      "epoch: 1 loss: 0.31775954365730286\n",
      "epoch: 2 loss: 0.26141199469566345\n",
      "epoch: 3 loss: 0.21578697860240936\n",
      "epoch: 4 loss: 0.18142224848270416\n",
      "epoch: 5 loss: 0.15617986023426056\n",
      "epoch: 6 loss: 0.13621804118156433\n",
      "epoch: 7 loss: 0.12166616320610046\n",
      "epoch: 8 loss: 0.10664428770542145\n",
      "epoch: 9 loss: 0.09684914350509644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|████▊                                                                           | 3/50 [07:38<1:59:07, 152.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= tensor(0.0001)\n",
      "epoch: 0 loss: 0.6618608832359314\n",
      "epoch: 1 loss: 0.3155348002910614\n",
      "epoch: 2 loss: 0.2544049024581909\n",
      "epoch: 3 loss: 0.20766378939151764\n",
      "epoch: 4 loss: 0.175788015127182\n",
      "epoch: 5 loss: 0.14920586347579956\n",
      "epoch: 6 loss: 0.13078856468200684\n",
      "epoch: 7 loss: 0.11626923829317093\n",
      "epoch: 8 loss: 0.10357780754566193\n",
      "epoch: 9 loss: 0.09434712678194046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██████▍                                                                         | 4/50 [10:17<1:58:05, 154.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= tensor(0.0001)\n",
      "epoch: 0 loss: 0.6352406740188599\n",
      "epoch: 1 loss: 0.30909615755081177\n",
      "epoch: 2 loss: 0.24739566445350647\n",
      "epoch: 3 loss: 0.1991301029920578\n",
      "epoch: 4 loss: 0.16666141152381897\n",
      "epoch: 5 loss: 0.14313487708568573\n",
      "epoch: 6 loss: 0.12389469146728516\n",
      "epoch: 7 loss: 0.10982787609100342\n",
      "epoch: 8 loss: 0.09792826324701309\n",
      "epoch: 9 loss: 0.08667797595262527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████                                                                        | 5/50 [12:43<1:53:39, 151.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= tensor(0.0002)\n",
      "epoch: 0 loss: 0.6137754321098328\n",
      "epoch: 1 loss: 0.29647642374038696\n",
      "epoch: 2 loss: 0.23391462862491608\n",
      "epoch: 3 loss: 0.19154486060142517\n",
      "epoch: 4 loss: 0.16032056510448456\n",
      "epoch: 5 loss: 0.13575603067874908\n",
      "epoch: 6 loss: 0.1181708574295044\n",
      "epoch: 7 loss: 0.10292201489210129\n",
      "epoch: 8 loss: 0.09188222885131836\n",
      "epoch: 9 loss: 0.0810440182685852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████████▌                                                                      | 6/50 [15:18<1:51:55, 152.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr= tensor(0.0002)\n",
      "epoch: 0 loss: 0.5994277596473694\n",
      "epoch: 1 loss: 0.3022109270095825\n",
      "epoch: 2 loss: 0.2426067739725113\n",
      "epoch: 3 loss: 0.19822770357131958\n",
      "epoch: 4 loss: 0.16294272243976593\n",
      "epoch: 5 loss: 0.13868972659111023\n",
      "epoch: 6 loss: 0.11935918033123016\n",
      "epoch: 7 loss: 0.10501895844936371\n",
      "epoch: 8 loss: 0.09305837750434875\n"
     ]
    }
   ],
   "source": [
    "lrs = 10**torch.linspace(-4,-2,50)\n",
    "losses=[]\n",
    "for lr in tqdm(lrs):\n",
    "    print('lr=',lr)\n",
    "    modellr = Model(28*28, 500, 10,1, [0,0,0,0,0],0,False)\n",
    "    loss=modellr.train(train_dl,epochs = 10, learning_rate=lr,lam=0)\n",
    "    loss=torch.tensor(loss).exp()\n",
    "    losses.append(min(loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1e5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lr vs losses:\n",
    "optimal_lr = lrs[losses.index(min(losses))]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(lrs,torch.tensor(losses))\n",
    "plt.xlabel('learning rates')\n",
    "plt.ylabel('exp(losses)')\n",
    "plt.axvline(x=optimal_lr,color='orange',linestyle='--',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc48824",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('optimal learning rate: \\n',optimal_lr.item())\n",
    "print('least training loss',min(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a1522",
   "metadata": {},
   "source": [
    "#### 2. correct regularization coefficient (lambda) \n",
    "\n",
    "Taking the optimal learning rate we now move on with similar experimental setup to find the optimal regularisation term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = 10**torch.linspace(-4,-2,50)\n",
    "losses=[]\n",
    "for lam in tqdm(lambdas):\n",
    "    modellam = Model(28*28, 500, 10,1, [0,0,0,0,0],0,True)\n",
    "    modellam.train(train_dl,epochs = 10, learning_rate=optimal_lr.item(),lam=lam)\n",
    "    loss = modellam.test(test_dl)\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75bf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lr vs losses:\n",
    "optimal_lam = lambdas[losses.index(min(losses))]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(lambdas,losses)\n",
    "plt.xlabel('lambdas')\n",
    "plt.ylabel('log(losses)')\n",
    "plt.axvline(x=optimal_lam,color='orange',linestyle='--',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6ad6fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'optimal lambda: \\n{optimal_lam.item():.7f}')\n",
    "print(f'least training loss: {min(losses):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c49d2f",
   "metadata": {},
   "source": [
    "#### 3. Shallow network without regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shallow non regularised\n",
    "model1 = Model(28*28, 500, 10,1, [0,0,0,0,0],0,False)\n",
    "model1.to(device)\n",
    "model1.train(train_dl,epochs = 100, learning_rate=optimal_lr,lam=optimal_lam)\n",
    "print(\"Loss on test:\")\n",
    "model1.test(test_dl)\n",
    "print(\"Training accuracy:\")\n",
    "get_accuracy(mnist_train,model1)\n",
    "print(\"Test accuracy:\")\n",
    "get_accuracy(mnist_test,model1)\n",
    "print(\"Training f1 score:\")\n",
    "get_f1(mnist_train,model1)\n",
    "print(\"Test f1 score:\")\n",
    "get_f1(mnist_test,model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e61d6",
   "metadata": {},
   "source": [
    "#### 4. Shallow network with regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d54a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model(28*28, 500, 10,1, [1,1,1,1,1],0.2,True)\n",
    "model2.to(device)\n",
    "model2.train(train_dl,epochs = 100, learning_rate=optimal_lr,lam=optimal_lam)\n",
    "print(\"Loss on test:\")\n",
    "model2.test(test_dl)\n",
    "print(\"Training accuracy:\")\n",
    "get_accuracy(mnist_train,model2)\n",
    "print(\"Test accuracy:\")\n",
    "get_accuracy(mnist_test,model2)\n",
    "print(\"Training f1 score:\")\n",
    "get_f1(mnist_train,model2)\n",
    "print(\"Test f1 score:\")\n",
    "get_f1(mnist_test,model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4cc3dd",
   "metadata": {},
   "source": [
    "#### 5. Deep network without regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Model(28*28, 500, 10,5, [0,0,0,0,0],0,False)\n",
    "model3.to(device)\n",
    "model3.train(train_dl,epochs = 100, learning_rate=optimal_lr,lam=optimal_lam)\n",
    "print(\"Loss on test:\")\n",
    "model3.test(test_dl)\n",
    "print(\"Training accuracy:\")\n",
    "get_accuracy(mnist_train,model3)\n",
    "print(\"Test accuracy:\")\n",
    "get_accuracy(mnist_test,model3)\n",
    "print(\"Training f1 score:\")\n",
    "get_f1(mnist_train,model3)\n",
    "print(\"Test f1 score:\")\n",
    "get_f1(mnist_test,model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe9c7ca",
   "metadata": {},
   "source": [
    "#### 6. Deep network with regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5840d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Model(28*28, 500, 10,5, [1,1,1,1,1],0.05,True)\n",
    "model4.to(device)\n",
    "model4.train(train_dl,epochs = 5, learning_rate=optimal_lr,lam=optimal_lam)\n",
    "print(\"Loss on test:\")\n",
    "model4.test(test_dl)\n",
    "print(\"Training accuracy:\")\n",
    "get_accuracy(mnist_train,model4)\n",
    "print(\"Test accuracy:\")\n",
    "get_accuracy(mnist_test,model4)\n",
    "print(\"Training f1 score:\")\n",
    "get_f1(mnist_train,model4)\n",
    "print(\"Test f1 score:\")\n",
    "get_f1(mnist_test,model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_f1(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e6e1e4",
   "metadata": {},
   "source": [
    "### 7.5.3 Get the best model! (1 + 1 point (bonus))\n",
    "\n",
    "* Present your model during a tutorial session. Justify your decisions when designing your model/solution.\n",
    "* If you achieve one of the top N results, you get yet another extra point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09604d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(xs[15].view(-1,28*28)).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c5dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(xs[15][0])\n",
    "print(ys[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a5f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7187e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
